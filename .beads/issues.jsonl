{"id":"jamma-05t","title":"Add covariate support to JAX path","description":"runner_jax.py raises NotImplementedError for covariates. Covariates are standard in GWAS (age, sex, PCs). Port covariate handling from NumPy path to JAX vectorized implementation.","status":"open","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:39.861195Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.552474Z"}
{"id":"jamma-0tm","title":"Add memory estimation before JAX allocation","description":"Before allocating large arrays in runner_jax.py, estimate required memory and warn/abort if exceeding available. Formula: (n_snps × n_samples × 6 × 8 bytes). Prevents OOM crashes.","status":"closed","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:44.347164Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.554776Z","closed_at":"2026-02-01T11:37:57.429793Z","close_reason":"Implemented in Phase 4: estimate_workflow_memory() in core/memory.py, integrated into runner_jax.py and kinship/compute.py with check_memory parameter"}
{"id":"jamma-3l4","title":"Add Performance section to README documenting acceleration","description":"Document why and where JAMMA uses JAX (GPU batching), Numba (CPU hot paths). Explain no Cython = simpler install. Set runtime expectations. Include brief comparison to pyGEMMA approach.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:54.742407Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.557212Z"}
{"id":"jamma-3vf","title":"Add memory pre-check inside eigendecompose_kinship","description":"**Problem:** scipy.linalg.eigh can OOM and trigger OS kill (segfault) before Python catches MemoryError. Current memory checks are at workflow entry, not at eigendecomp call site.\n\n**Root cause:** 100k x 100k eigendecomp on Databricks caused silent restart. Memory estimation exists but eigendecomp function has no guard.\n\n**Fix:** Add check_memory_available() call at start of eigendecompose_kinship() before calling scipy:\n- Required: K (input n²×8) + eigenvectors (output n²×8) + workspace (~26n×8)\n- For 100k: ~240GB peak\n\n**Acceptance:**\n- [ ] eigendecompose_kinship raises MemoryError before scipy call if insufficient\n- [ ] Error message shows required vs available GB\n- [ ] Test: mock available=50GB, attempt 100k eigendecomp → MemoryError","status":"open","priority":1,"issue_type":"bug","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:29:53.157363Z","created_by":"Michael Denyer","updated_at":"2026-02-02T11:29:53.157363Z","dependencies":[{"issue_id":"jamma-3vf","depends_on_id":"jamma-8ke","type":"blocks","created_at":"2026-02-02T11:30:54.355633Z","created_by":"Michael Denyer"}]}
{"id":"jamma-4so","title":"Numba optimization for NumPy LMM path","description":"Consider using Numba JIT compilation to accelerate the NumPy LMM path (compute_Uab, calc_pab).\n\n## Context\n- Current NumPy path runs at ~24s for mouse_hs1940 dataset\n- JAX path runs at ~4.6s (5x faster) \n- Numba could potentially match or beat JAX speed on CPU\n\n## Trade-offs to evaluate\n- Numba adds compilation overhead on first call\n- JAX already provides excellent performance and GPU support\n- Numba would only help CPU-bound workloads\n- Additional dependency to maintain\n\n## Potential approach\n- Add @numba.jit decorators to inner loop functions\n- Use nopython mode for maximum speed\n- Benchmark against JAX path to determine if worthwhile\n\n## Priority\nLow - JAX path is already 4.2x faster than GEMMA. This is a nice-to-have optimization for users who cannot use JAX.","status":"closed","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T00:47:49.91587Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.560622Z","closed_at":"2026-02-01T01:09:04.451098Z","close_reason":"Implemented Numba JIT compilation for NumPy LMM path, achieving 3.4x speedup"}
{"id":"jamma-5nc","title":"Mock-based memory check tests for full LMM workflow","description":"Add tests that mock psutil.virtual_memory to verify memory checks work at each stage: genotype loading, eigendecomp, LMM runtime. Tests should verify MemoryError is raised with clear diagnostics before allocation, not after OOM.","status":"open","priority":1,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:53.820895Z","created_by":"Michael Denyer","updated_at":"2026-02-02T13:29:53.820895Z","dependencies":[{"issue_id":"jamma-5nc","depends_on_id":"jamma-790","type":"blocks","created_at":"2026-02-02T13:29:58.345725Z","created_by":"Michael Denyer"},{"issue_id":"jamma-5nc","depends_on_id":"jamma-bv5","type":"blocks","created_at":"2026-02-02T13:29:58.495893Z","created_by":"Michael Denyer"}]}
{"id":"jamma-66g","title":"Implement tiered test strategy for faster feedback","description":"**Problem:** Long-running scale tests on Databricks give slow feedback. Bugs found late. Need faster local iteration.\n\n**Proposed test tiers:**\n\n**Tier 0: Fast (1-5s, always run)**\n- Small synthetic: 50 samples × 200 SNPs\n- Cross-path parity: NumPy vs JAX vs streaming (tiny data)\n- Edge cases: MAF/missingness boundaries, degenerate SNPs\n- Memory estimation unit tests (no allocation)\n\n**Tier 1: Parity (optional/nightly, ~30s)**\n- GEMMA fixture comparison (1k × 10k)\n- Covariate validation against reference\n- Output format validation\n\n**Tier 2: Scale/Memory (manual/nightly, minutes)**\n- Synthetic large sizes to validate chunking\n- Memory estimator accuracy at 10k, 50k, 100k (mocked available memory)\n- Result streaming memory profile\n\n**Implementation:**\n- pytest markers: @pytest.mark.tier0, @pytest.mark.tier1, @pytest.mark.tier2\n- Default: run tier0 + tier1\n- CI: tier0 only for PR checks, tier1 nightly\n- Local: tier0 + tier1, tier2 opt-in\n\n**Debug mode shortcuts:**\n- Reduce n_grid (50→5) and n_refine (10→2) for fast iteration\n- Add --fast-debug CLI flag\n\n**Acceptance:**\n- [ ] Markers defined and documented\n- [ ] pytest.ini configured for default tier0+tier1\n- [ ] CI updated for tier-based runs\n- [ ] README documents test tiers","status":"open","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:30:36.301313Z","created_by":"Michael Denyer","updated_at":"2026-02-02T11:30:36.301313Z"}
{"id":"jamma-69k","title":"Add tests for heterogeneous missingness patterns","description":"Test edge cases: SNPs with varying missing rates, samples with high missingness, patterns that stress imputation. Currently only all-missing SNP case was fixed but not tested.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:46.106351Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.558624Z"}
{"id":"jamma-790","title":"Pre-flight memory check for LMM with pre-computed kinship (-k flag)","description":"When running LMM with -k flag (pre-computed kinship), eigendecomp memory check is bypassed. Need separate pre-flight check for LMM-only path that verifies U matrix + chunk buffers fit in memory. Must be testable with mocked psutil.virtual_memory.","status":"open","priority":1,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:48.655272Z","created_by":"Michael Denyer","updated_at":"2026-02-02T13:29:48.655272Z","dependencies":[{"issue_id":"jamma-790","depends_on_id":"jamma-84t","type":"blocks","created_at":"2026-02-02T13:29:58.046224Z","created_by":"Michael Denyer"}]}
{"id":"jamma-84t","title":"Add estimate_lmm_memory() standalone helper","description":"Add standalone helper like estimate_eigendecomp_memory() but for LMM runtime: U matrix + eigenvalues + UtW/Uty + chunk buffers. Allows users to query memory needs without running workflow.","status":"open","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:51.83224Z","created_by":"Michael Denyer","updated_at":"2026-02-02T13:29:51.83224Z"}
{"id":"jamma-8ke","title":"Add RSS memory instrumentation for debugging","description":"**Problem:** When OOM occurs, no visibility into WHERE peak memory happened. Silent restart gives no diagnostic info.\n\n**Requested instrumentation:**\n1. Log RSS at key workflow points (kinship complete, eigendecomp complete, each LMM chunk)\n2. Track peak RSS throughout run\n3. Optional: periodic RSS sampling (every N seconds)\n\n**Implementation:**\n- Add debug_memory flag to workflow functions\n- Use psutil.Process().memory_info().rss\n- Log format: `[MEMORY] {phase}: {rss_gb:.1f}GB (peak: {peak_gb:.1f}GB)`\n\n**For GPU debugging:**\n- Set XLA_PYTHON_CLIENT_PREALLOCATE=false\n- Log JAX device memory stats if available\n\n**Acceptance:**\n- [ ] debug_memory=True logs RSS at each phase boundary\n- [ ] Peak RSS tracked and reported at workflow end\n- [ ] Test: verify logs appear with expected format","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:30:21.252726Z","created_by":"Michael Denyer","updated_at":"2026-02-02T11:30:21.252726Z"}
{"id":"jamma-8yc","title":"Stream LMM results to disk instead of buffering","description":"**Problem:** All LMM functions return list[AssocResult] which accumulates entire result set in memory. At 95k SNPs with extended output, this can be significant.\n\n**Current behavior:**\n- run_lmm_association_streaming() returns list[AssocResult]\n- Results appended per-chunk, then returned as full list\n- Caller writes to disk after getting full list\n\n**Impact:** At scale (200k samples × 95k SNPs), result buffering adds memory pressure on top of eigendecomp/JAX buffers.\n\n**Fix options:**\n1. Generator pattern: yield results per-chunk, caller writes incrementally\n2. Direct-to-disk: accept output file path, write per-chunk, return count\n3. Hybrid: stream to temp file, return path\n\n**Acceptance:**\n- [ ] Results written incrementally (not buffered)\n- [ ] Memory profile shows flat result memory regardless of SNP count\n- [ ] Existing API preserved (option 1 or wrapper)\n- [ ] Test: 100k SNPs, RSS stays bounded","status":"open","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:30:06.103556Z","created_by":"Michael Denyer","updated_at":"2026-02-02T11:30:06.103556Z","dependencies":[{"issue_id":"jamma-8yc","depends_on_id":"jamma-3vf","type":"blocks","created_at":"2026-02-02T11:30:54.031908Z","created_by":"Michael Denyer"},{"issue_id":"jamma-8yc","depends_on_id":"jamma-8ke","type":"blocks","created_at":"2026-02-02T11:30:54.185717Z","created_by":"Michael Denyer"}]}
{"id":"jamma-b1k","title":"Add SNP chunking to JAX LMM runner for large-scale datasets","description":"Current JAX runner materializes (n_snps, n_samples, 6) arrays which is infeasible for large cohorts. For 200K samples × 95K SNPs, Uab alone requires ~912GB. Need to add an outer chunking loop that processes SNPs in batches (e.g., 5000-10000 at a time) and concatenates results. This will enable biobank-scale GWAS while preserving JAX performance benefits within each chunk.","status":"closed","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T01:22:42.545523Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.555414Z","closed_at":"2026-02-01T02:54:06.06675Z","close_reason":"Duplicate of GEMMA-xfq (chunked SNP processing)"}
{"id":"jamma-bv5","title":"Pre-flight memory check for genotype loading","description":"Genotypes are loaded fully into memory before chunking in LMM. Need pre-flight check that estimates genotype array size (n_samples × n_snps × 8 bytes) and raises MemoryError if insufficient. Must be testable with mocked psutil.virtual_memory.","status":"open","priority":1,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:50.353896Z","created_by":"Michael Denyer","updated_at":"2026-02-02T13:29:50.353896Z","dependencies":[{"issue_id":"jamma-bv5","depends_on_id":"jamma-84t","type":"blocks","created_at":"2026-02-02T13:29:58.196827Z","created_by":"Michael Denyer"}]}
{"id":"jamma-c3q","title":"Evaluate chex for JAX runtime assertions","description":"Consider adding chex library for runtime shape/type assertions in JAX code. Would provide chex.assert_shape(), chex.assert_rank(), chex.assert_type(), and @chex.dataclass for frozen JAX-compatible dataclasses. Evaluate trade-offs: better error messages vs runtime overhead and additional dependency.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T12:51:07.170144Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.556215Z"}
{"id":"jamma-czl","title":"Add Newton refinement to lambda optimization","description":"After Brent's method finds approximate root, use Newton-Raphson with second derivative for faster convergence. pyGEMMA uses: brentq(rtol=0.1) → newton(rtol=1e-5, maxiter=10). May improve edge case convergence.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:34.575018Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.55999Z"}
{"id":"jamma-f00","title":"Optimize JAX chunking for large sample counts","description":"At 200K samples, chunk size drops to 1,416 SNPs (68 chunks for 95K SNPs). Each chunk incurs overhead for CPU rotation, device transfer, and result transfer.\n\nCurrent constraint: Uab array (n_snps, n_samples, 6) must stay under 1.7B elements to avoid int32 overflow.\n\nPotential optimizations:\n1. Stream Uab computation - don't store full array, compute Pab incrementally\n2. Fuse rotation with Uab computation to avoid UtG intermediate\n3. Process chunks in parallel threads (multiple JAX streams)\n4. Investigate JAX int64 indexing options (XLA_FLAGS)\n\nBenchmark data (old run, before scipy eigendecomp fix):\n- 10K samples: 78 SNPs/sec (vs GEMMA 891 SNPs/sec) - 11x slower\n- Chunk sizes: 200K→1,416, 100K→2,833, 50K→5,666, 10K→28,333","status":"open","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T01:20:04.780291Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.554123Z"}
{"id":"jamma-irh","title":"Add dimension sanity checks for corrupted PLINK files","description":"Input validation in plink.py - check genotype dimensions match .fam/.bim counts, validate value ranges (0,1,2,NaN only), warn on suspicious patterns. bed-reader handles parsing but not semantic validation.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:42.449529Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.559595Z"}
{"id":"jamma-iw5","title":"Add second public validation dataset (1000 Genomes subset)","description":"Strengthen equivalence story with second dataset beyond mouse_hs1940. Use 1000 Genomes subset (publicly available, no data agreements). Compare stats + p-value distributions vs GEMMA.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:56.285063Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.556898Z"}
{"id":"jamma-kdd","title":"Add benchmark harness with reproducible perf comparisons","description":"Create experiments/benchmarks/ with scripts to compare JAMMA vs GEMMA/GCTA/fastGWA. Include frozen data subsets, automated plots (time vs samples, time vs SNPs). Requires access to hardware with sufficient memory for meaningful benchmarks.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:51.386942Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.558141Z"}
{"id":"jamma-qfk","title":"Add top-level jamma.gwas() convenience API","description":"Single blessed entry point: from jamma import gwas. Wraps kinship computation + LMM association in one call. Reduces onboarding friction. Keep existing granular API for advanced users.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:52.813156Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.554454Z"}
{"id":"jamma-rwo","title":"Add grid search fallback for lambda optimization","description":"When Brent optimization fails or hits bounds, fall back to evaluating likelihood at discrete lambda values (10^-5 to 10^5 in log steps). Useful for diagnostics and pathological cases. pyGEMMA has grid=True option.","status":"open","priority":4,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:36.21734Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.561229Z"}
{"id":"jamma-wr2","title":"Wire -maf/-miss filters in CLI","description":"README documents -maf and -miss CLI options but they aren't exposed in cli.py. The filtering logic exists (maf_threshold/miss_threshold params in run_lmm_association) but CLI doesn't wire them up.","status":"closed","priority":3,"issue_type":"bug","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T03:19:49.736363Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.555726Z","closed_at":"2026-02-02T03:21:01.295041Z","close_reason":"Not a bug - -maf and -miss ARE wired in CLI. Verified with 'jamma lmm --help' showing both options."}
{"id":"jamma-xfq","title":"Implement chunked SNP processing for large datasets","description":"Process SNPs in chunks to bound memory usage. Required for 500K SNPs at 200K samples where full array would be 4.8TB. Track progress across chunks, allow resume on failure.","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:37.887528Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.553693Z","closed_at":"2026-02-01T15:26:43.190349Z","close_reason":"Implemented in runner_jax.py with _compute_chunk_size() and chunked processing loop"}
{"id":"jamma-zq8","title":"Add --precision flag for float32/float64 choice","description":"Add CLI option to choose precision. Float32 halves memory (critical for 200K samples) but may reduce numerical precision. Default to float64 for safety, allow float32 for memory-constrained environments. Hybrid approach: float32 for storage, float64 for REML computation.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:32.80927Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.555097Z"}
