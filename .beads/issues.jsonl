{"id":"jamma-05t","title":"Add covariate support to JAX path","description":"runner_jax.py raises NotImplementedError for covariates. Covariates are standard in GWAS (age, sex, PCs). Port covariate handling from NumPy path to JAX vectorized implementation.","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:39.861195Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:11.801128Z","closed_at":"2026-02-05T23:39:11.801128Z","close_reason":"Duplicate of jamma-qth - covariate support fully implemented in JAX runner"}
{"id":"jamma-0cc","title":"Lambda optimization bounds (-lmin, -lmax, -region)","description":"Expose lambda optimization bounds as CLI parameters. Currently hardcoded: lmin=1e-5, lmax=1e+5, 50 grid points. GEMMA allows user override for edge cases where default bounds are too narrow. Low complexity: thread parameters through to golden_section_optimize_lambda.","status":"open","priority":4,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:03:02.696457Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:03:02.696457Z"}
{"id":"jamma-0tm","title":"Add memory estimation before JAX allocation","description":"Before allocating large arrays in runner_jax.py, estimate required memory and warn/abort if exceeding available. Formula: (n_snps × n_samples × 6 × 8 bytes). Prevents OOM crashes.","status":"closed","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:44.347164Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.554776Z","closed_at":"2026-02-01T11:37:57.429793Z","close_reason":"Implemented in Phase 4: estimate_workflow_memory() in core/memory.py, integrated into runner_jax.py and kinship/compute.py with check_memory parameter"}
{"id":"jamma-15w","title":"Category covariates (-cat)","description":"Support categorical covariate file input. GEMMA encodes categories as indicator variables internally. Low complexity: one-hot encoding of categorical columns before W matrix construction. Low priority - users can pre-encode categories.","status":"open","priority":4,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:59.917287Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:59.917287Z"}
{"id":"jamma-2dj","title":"Extend JAX runner to support all LMM test modes (-lmm 1/2/3/4)","description":"The JAX runner (run_lmm_association_jax) and streaming runner (run_lmm_association_streaming) only support Wald test (-lmm 1). LRT (-lmm 2), Score (-lmm 3), and All-tests (-lmm 4) are only available via the slow NumPy per-SNP loop.\n\nThe JAX batch path uses vmap to parallelize across SNPs and grid+golden section for lambda optimization — this is where the advertised 4x speedup comes from. But CLI users get the NumPy path for all modes.\n\nWork needed:\n1. Add MLE log-likelihood to likelihood_jax.py (batch version of mle_log_likelihood)\n2. Add batch null model computation (compute_null_model_mle as JAX)  \n3. Add batch score test (calc_score_test as vmap-able)\n4. Add batch LRT (calc_lrt_test as vmap-able)\n5. Add lmm_mode parameter to run_lmm_association_jax() and run_lmm_association_streaming()\n6. Validate all modes against GEMMA fixtures (extend test_runner_jax.py)\n7. Update prove_equivalence.py to test JAX path for all modes","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-05T10:53:54.743158Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:07.810035Z","closed_at":"2026-02-05T23:39:07.810035Z","close_reason":"Implemented - all 4 LMM modes (Wald/LRT/Score/All) supported in both full-load and streaming runners"}
{"id":"jamma-2x9","title":"Memory leak in sequential benchmark runs causes SIGSEGV at 100k samples","description":"## Problem\n\nRunning sequential benchmark runs in a Databricks notebook causes SIGSEGV at 100k samples.\n\n### Evidence from logs:\n- Run 1 (10k × 100k): Completes\n- Run 2 (50k × 10k): Completes, RSS at 89.88GB after all chunks\n- Run 3 (100k × 100k): SIGSEGV during/after eigendecomposition\n\n### Root Cause Analysis:\nMemory from previous runs is NOT being freed between benchmark iterations:\n1. Python GC doesn't immediately free large NumPy/JAX arrays\n2. JAX JIT caches retain compiled functions and their buffer references\n3. Cumulative memory: 89GB (from run 2) + 160GB (run 3 eigendecomp) ≈ 250GB+\n4. OOM manifests as SIGSEGV when scipy.linalg.eigh tries to allocate workspace\n\n### Memory estimate for 100k × 100k:\n- Kinship: 80GB\n- Eigenvectors: 80GB  \n- Eigendecomp workspace: 0.02GB\n- Peak (eigendecomp): 160GB alone\n\n### Solution:\nAdd explicit memory cleanup between benchmark runs:\n1. `del` large arrays (kinship, eigenvectors, genotypes)\n2. `gc.collect()` to force garbage collection\n3. `jax.clear_caches()` to free JIT compilation caches\n4. Consider `jax.clear_backends()` for complete device memory cleanup\n\n### Affected Code:\n- Benchmark notebooks need cleanup between runs\n- Consider adding cleanup helper function to jamma API\n\n### Workaround:\nRestart Python kernel between large benchmark runs.","status":"closed","priority":1,"issue_type":"bug","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T19:23:11.373201Z","created_by":"Michael Denyer","updated_at":"2026-02-03T12:53:07.858303Z","closed_at":"2026-02-03T12:53:07.858303Z","close_reason":"Rust/faer backend avoids OpenBLAS threading bugs that caused SIGSEGV. Memory cleanup still good practice but no longer causes crashes."}
{"id":"jamma-32k","title":"BLUP prediction (-predict)","description":"Implement Best Linear Unbiased Prediction. -predict 1: BLUP genomic prediction using estimated SNP effects. -predict 2: Cross-validation for model assessment. Outputs predicted phenotype values. Medium complexity: uses estimated variance components and kinship to compute breeding values.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:55.647428Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:55.647428Z"}
{"id":"jamma-3l4","title":"Add Performance section to README documenting acceleration","description":"Document why and where JAMMA uses JAX (GPU batching), Numba (CPU hot paths). Explain no Cython = simpler install. Set runtime expectations. Include brief comparison to pyGEMMA approach.","status":"closed","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:54.742407Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:17.542446Z","closed_at":"2026-02-05T23:39:17.542446Z","close_reason":"Implemented - README.md has Performance section with GEMMA vs JAMMA benchmark table"}
{"id":"jamma-3vf","title":"Add memory pre-check inside eigendecompose_kinship","description":"**Problem:** scipy.linalg.eigh can OOM and trigger OS kill (segfault) before Python catches MemoryError. Current memory checks are at workflow entry, not at eigendecomp call site.\n\n**Root cause:** 100k x 100k eigendecomp on Databricks caused silent restart. Memory estimation exists but eigendecomp function has no guard.\n\n**Fix:** Add check_memory_available() call at start of eigendecompose_kinship() before calling scipy:\n- Required: K (input n²×8) + eigenvectors (output n²×8) + workspace (~26n×8)\n- For 100k: ~240GB peak\n\n**Acceptance:**\n- [ ] eigendecompose_kinship raises MemoryError before scipy call if insufficient\n- [ ] Error message shows required vs available GB\n- [ ] Test: mock available=50GB, attempt 100k eigendecomp → MemoryError","notes":"Rust backend reduces urgency - faer doesn't have OpenBLAS bugs. Memory pre-check still useful but P2 now.","status":"closed","priority":2,"issue_type":"bug","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:29:53.157363Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:15.98911Z","closed_at":"2026-02-05T23:39:15.98911Z","close_reason":"Implemented - eigen.py:131-138 calls estimate_eigendecomp_memory() + check_memory_available()","dependencies":[{"issue_id":"jamma-3vf","depends_on_id":"jamma-8ke","type":"blocks","created_at":"2026-02-02T11:30:54.355633Z","created_by":"Michael Denyer"}]}
{"id":"jamma-4so","title":"Numba optimization for NumPy LMM path","description":"Consider using Numba JIT compilation to accelerate the NumPy LMM path (compute_Uab, calc_pab).\n\n## Context\n- Current NumPy path runs at ~24s for mouse_hs1940 dataset\n- JAX path runs at ~4.6s (5x faster) \n- Numba could potentially match or beat JAX speed on CPU\n\n## Trade-offs to evaluate\n- Numba adds compilation overhead on first call\n- JAX already provides excellent performance and GPU support\n- Numba would only help CPU-bound workloads\n- Additional dependency to maintain\n\n## Potential approach\n- Add @numba.jit decorators to inner loop functions\n- Use nopython mode for maximum speed\n- Benchmark against JAX path to determine if worthwhile\n\n## Priority\nLow - JAX path is already 4.2x faster than GEMMA. This is a nice-to-have optimization for users who cannot use JAX.","status":"closed","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T00:47:49.91587Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.560622Z","closed_at":"2026-02-01T01:09:04.451098Z","close_reason":"Implemented Numba JIT compilation for NumPy LMM path, achieving 3.4x speedup"}
{"id":"jamma-56h","title":"Standardized kinship (-gk 2)","description":"Implement standardized kinship matrix computation (GEMMA -gk 2). Currently JAMMA warns and falls back to mode 1. v1.3 will change the warning to an error; this issue implements the actual computation. Formula: (X - mean) / sqrt(p*(1-p)) before outer product.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:19.823278Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:19.823278Z"}
{"id":"jamma-5nc","title":"Mock-based memory check tests for full LMM workflow","description":"Add tests that mock psutil.virtual_memory to verify memory checks work at each stage: genotype loading, eigendecomp, LMM runtime. Tests should verify MemoryError is raised with clear diagnostics before allocation, not after OOM.","status":"closed","priority":1,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:53.820895Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:25.648983Z","closed_at":"2026-02-05T23:39:25.648983Z","close_reason":"Implemented - test_cli_memory.py, test_memory.py, test_eigendecomp_memory.py cover pre-flight checks","dependencies":[{"issue_id":"jamma-5nc","depends_on_id":"jamma-790","type":"blocks","created_at":"2026-02-02T13:29:58.345725Z","created_by":"Michael Denyer"},{"issue_id":"jamma-5nc","depends_on_id":"jamma-bv5","type":"blocks","created_at":"2026-02-02T13:29:58.495893Z","created_by":"Michael Denyer"}]}
{"id":"jamma-66g","title":"Implement tiered test strategy for faster feedback","description":"**Problem:** Long-running scale tests on Databricks give slow feedback. Bugs found late. Need faster local iteration.\n\n**Proposed test tiers:**\n\n**Tier 0: Fast (1-5s, always run)**\n- Small synthetic: 50 samples × 200 SNPs\n- Cross-path parity: NumPy vs JAX vs streaming (tiny data)\n- Edge cases: MAF/missingness boundaries, degenerate SNPs\n- Memory estimation unit tests (no allocation)\n\n**Tier 1: Parity (optional/nightly, ~30s)**\n- GEMMA fixture comparison (1k × 10k)\n- Covariate validation against reference\n- Output format validation\n\n**Tier 2: Scale/Memory (manual/nightly, minutes)**\n- Synthetic large sizes to validate chunking\n- Memory estimator accuracy at 10k, 50k, 100k (mocked available memory)\n- Result streaming memory profile\n\n**Implementation:**\n- pytest markers: @pytest.mark.tier0, @pytest.mark.tier1, @pytest.mark.tier2\n- Default: run tier0 + tier1\n- CI: tier0 only for PR checks, tier1 nightly\n- Local: tier0 + tier1, tier2 opt-in\n\n**Debug mode shortcuts:**\n- Reduce n_grid (50→5) and n_refine (10→2) for fast iteration\n- Add --fast-debug CLI flag\n\n**Acceptance:**\n- [ ] Markers defined and documented\n- [ ] pytest.ini configured for default tier0+tier1\n- [ ] CI updated for tier-based runs\n- [ ] README documents test tiers","status":"closed","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:30:36.301313Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:14.789905Z","closed_at":"2026-02-05T23:39:14.789905Z","close_reason":"Implemented - tier0/tier1/tier2 markers in pyproject.toml with selective pytest execution"}
{"id":"jamma-678","title":"Variance component estimation (-vc)","description":"Implement GEMMA's variance component estimation modes. -vc 1: Haseman-Elston regression (fast, approximate). -vc 2: REML-based variance component estimation (accurate, slower). Outputs heritability (PVE), genetic variance, and residual variance. Foundation for partitioned heritability analysis. Related flags: -windowbp, -windowcm, -windowns for windowed estimation.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:34.156487Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:34.156487Z"}
{"id":"jamma-69k","title":"Add tests for heterogeneous missingness patterns","description":"Test edge cases: SNPs with varying missing rates, samples with high missingness, patterns that stress imputation. Currently only all-missing SNP case was fixed but not tested.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:46.106351Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.558624Z"}
{"id":"jamma-6rk","title":"Multivariate LMM (mvLMM)","description":"Joint analysis of multiple phenotypes simultaneously. GEMMA's mvLMM uses multivariate normal likelihood with cross-phenotype variance-covariance estimation. Very high complexity: requires new optimizer (Newton-Raphson/PX-EM), variance-covariance matrix handling, and different likelihood function. Unique differentiator if implemented with JAX acceleration. Related flags: -n for phenotype selection, -nri, -emi, -pnr, -nrp, -emp for convergence control.","status":"open","priority":2,"issue_type":"epic","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:31.257882Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:31.257882Z"}
{"id":"jamma-790","title":"Pre-flight memory check for LMM with pre-computed kinship (-k flag)","description":"When running LMM with -k flag (pre-computed kinship), eigendecomp memory check is bypassed. Need separate pre-flight check for LMM-only path that verifies U matrix + chunk buffers fit in memory. Must be testable with mocked psutil.virtual_memory.","status":"closed","priority":1,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:48.655272Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:24.222638Z","closed_at":"2026-02-05T23:39:24.222638Z","close_reason":"Implemented - cli.py:283-315 checks memory before loading pre-computed kinship with -k flag","dependencies":[{"issue_id":"jamma-790","depends_on_id":"jamma-84t","type":"blocks","created_at":"2026-02-02T13:29:58.046224Z","created_by":"Michael Denyer"}]}
{"id":"jamma-84t","title":"Add estimate_lmm_memory() standalone helper","description":"Add standalone helper like estimate_eigendecomp_memory() but for LMM runtime: U matrix + eigenvalues + UtW/Uty + chunk buffers. Allows users to query memory needs without running workflow.","status":"closed","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:51.83224Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:13.519038Z","closed_at":"2026-02-05T23:39:13.519038Z","close_reason":"Implemented as estimate_workflow_memory() and estimate_streaming_memory() in core/memory.py"}
{"id":"jamma-8f5","title":"HWE filter (-hwe)","description":"Add Hardy-Weinberg Equilibrium p-value threshold filter. SNPs with HWE p-value below threshold are excluded (indicates genotyping error or population stratification). Low complexity: chi-squared test on genotype counts (AA, AB, BB) vs expected frequencies under HWE. Adds to existing SNP filtering pipeline.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:38.637321Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:38.637321Z"}
{"id":"jamma-8ke","title":"Add RSS memory instrumentation for debugging","description":"**Problem:** When OOM occurs, no visibility into WHERE peak memory happened. Silent restart gives no diagnostic info.\n\n**Requested instrumentation:**\n1. Log RSS at key workflow points (kinship complete, eigendecomp complete, each LMM chunk)\n2. Track peak RSS throughout run\n3. Optional: periodic RSS sampling (every N seconds)\n\n**Implementation:**\n- Add debug_memory flag to workflow functions\n- Use psutil.Process().memory_info().rss\n- Log format: `[MEMORY] {phase}: {rss_gb:.1f}GB (peak: {peak_gb:.1f}GB)`\n\n**For GPU debugging:**\n- Set XLA_PYTHON_CLIENT_PREALLOCATE=false\n- Log JAX device memory stats if available\n\n**Acceptance:**\n- [ ] debug_memory=True logs RSS at each phase boundary\n- [ ] Peak RSS tracked and reported at workflow end\n- [ ] Test: verify logs appear with expected format","status":"closed","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:30:21.252726Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:15.181506Z","closed_at":"2026-02-05T23:39:15.181506Z","close_reason":"Implemented - log_rss_memory() in utils/logging.py, used throughout runner and eigen"}
{"id":"jamma-8yc","title":"Stream LMM results to disk instead of buffering","description":"**Problem:** All LMM functions return list[AssocResult] which accumulates entire result set in memory. At 95k SNPs with extended output, this can be significant.\n\n**Current behavior:**\n- run_lmm_association_streaming() returns list[AssocResult]\n- Results appended per-chunk, then returned as full list\n- Caller writes to disk after getting full list\n\n**Impact:** At scale (200k samples × 95k SNPs), result buffering adds memory pressure on top of eigendecomp/JAX buffers.\n\n**Fix options:**\n1. Generator pattern: yield results per-chunk, caller writes incrementally\n2. Direct-to-disk: accept output file path, write per-chunk, return count\n3. Hybrid: stream to temp file, return path\n\n**Acceptance:**\n- [ ] Results written incrementally (not buffered)\n- [ ] Memory profile shows flat result memory regardless of SNP count\n- [ ] Existing API preserved (option 1 or wrapper)\n- [ ] Test: 100k SNPs, RSS stays bounded","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T11:30:06.103556Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:24.923399Z","closed_at":"2026-02-05T23:39:24.923399Z","close_reason":"Implemented - IncrementalAssocWriter in lmm/io.py streams per-SNP results to disk","dependencies":[{"issue_id":"jamma-8yc","depends_on_id":"jamma-3vf","type":"blocks","created_at":"2026-02-02T11:30:54.031908Z","created_by":"Michael Denyer"},{"issue_id":"jamma-8yc","depends_on_id":"jamma-8ke","type":"blocks","created_at":"2026-02-02T11:30:54.185717Z","created_by":"Michael Denyer"}]}
{"id":"jamma-9vh","title":"Document dual scipy/Rust eigendecomp backend","description":"## Current State\n\nRust/faer backend implemented for eigendecomp:\n- `jamma_core.eigendecompose_kinship()` via PyO3\n- Auto-selected when no GPU (Rust) or GPU available (scipy for JAX compat)\n- `JAMMA_BACKEND=rust|jax` env override\n- CI builds wheels for Linux/macOS\n\n## What's Working\n- Backend detection in `src/jamma/core/backend.py`\n- Dispatch in `src/jamma/lmm/eigen.py`\n- Rust tests in `rust/jamma-core/tests/`\n- Wheel builds via GitHub Actions\n\n## Still Needed\n- [ ] Add scipy backend option explicitly (currently \"jax\" uses scipy)\n- [ ] Update CLI --version to show backend info\n- [ ] Add --backend flag to CLI\n- [ ] Document backend selection in README\n- [ ] Benchmark comparison in docs","status":"closed","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-03T12:55:34.375005Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:12.91614Z","closed_at":"2026-02-05T23:39:12.91614Z","close_reason":"Obsolete - Rust/faer backend abandoned (too much memory overhead). Only numpy/LAPACK backend with MKL support"}
{"id":"jamma-9yk","title":"GxE interaction testing (-gxe)","description":"Implement genotype-by-environment interaction testing. Tests whether SNP effects differ across environment/exposure levels. Takes environment factor file as input. Modifies the LMM likelihood to include SNP x environment interaction term. Medium complexity: extends existing likelihood functions rather than replacing them.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:36.225514Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:36.225514Z"}
{"id":"jamma-a5l","title":"Pre-computed eigendecomposition input (-d, -u)","description":"Accept pre-computed eigenvalues (-d) and eigenvectors (-u) files to skip eigendecomposition. Eigendecomp is O(n^3) and dominates runtime for large n. Reusing pre-computed eigen across multiple phenotype analyses saves significant time. GEMMA uses space-separated text format for both.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:22.767828Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:22.767828Z"}
{"id":"jamma-acc","title":"SNP correlation matrix (-calccor)","description":"Compute pairwise SNP correlation (LD/r^2) matrix. Used for meta-analysis and summary-statistic based methods. Medium complexity: O(p^2) computation on genotype matrix. Low priority - specialized use case, many dedicated LD tools exist.","status":"open","priority":4,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:58.224865Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:58.224865Z"}
{"id":"jamma-b1k","title":"Add SNP chunking to JAX LMM runner for large-scale datasets","description":"Current JAX runner materializes (n_snps, n_samples, 6) arrays which is infeasible for large cohorts. For 200K samples × 95K SNPs, Uab alone requires ~912GB. Need to add an outer chunking loop that processes SNPs in batches (e.g., 5000-10000 at a time) and concatenates results. This will enable biobank-scale GWAS while preserving JAX performance benefits within each chunk.","status":"closed","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T01:22:42.545523Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.555414Z","closed_at":"2026-02-01T02:54:06.06675Z","close_reason":"Duplicate of GEMMA-xfq (chunked SNP processing)"}
{"id":"jamma-bv5","title":"Pre-flight memory check for genotype loading","description":"Genotypes are loaded fully into memory before chunking in LMM. Need pre-flight check that estimates genotype array size (n_samples × n_snps × 8 bytes) and raises MemoryError if insufficient. Must be testable with mocked psutil.virtual_memory.","status":"closed","priority":1,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T13:29:50.353896Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:23.637073Z","closed_at":"2026-02-05T23:39:23.637073Z","close_reason":"Implemented - estimate_workflow_memory() and check_memory_available() called before genotype loading","dependencies":[{"issue_id":"jamma-bv5","depends_on_id":"jamma-84t","type":"blocks","created_at":"2026-02-02T13:29:58.196827Z","created_by":"Michael Denyer"}]}
{"id":"jamma-c3q","title":"Evaluate chex for JAX runtime assertions","description":"Consider adding chex library for runtime shape/type assertions in JAX code. Would provide chex.assert_shape(), chex.assert_rank(), chex.assert_type(), and @chex.dataclass for frozen JAX-compatible dataclasses. Evaluate trade-offs: better error messages vs runtime overhead and additional dependency.","status":"closed","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T12:51:07.170144Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:30.21156Z","closed_at":"2026-02-05T23:39:30.21156Z","close_reason":"Closing - manual validation sufficient, chex adds dependency for marginal benefit"}
{"id":"jamma-cs6","title":"Individual and SNP weights (-widv, -wsnp)","description":"Support per-individual residual variance weights (-widv) and per-SNP weights (-wsnp) in LMM analysis. Individual weights adjust for heteroscedastic residuals. SNP weights adjust for LD or functional annotation. Both modify the likelihood computation. Low-medium complexity.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:43.769294Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:43.769294Z"}
{"id":"jamma-czl","title":"Add Newton refinement to lambda optimization","description":"After Brent's method finds approximate root, use Newton-Raphson with second derivative for faster convergence. pyGEMMA uses: brentq(rtol=0.1) → newton(rtol=1e-5, maxiter=10). May improve edge case convergence.","status":"closed","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:34.575018Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:19.142674Z","closed_at":"2026-02-05T23:39:19.142674Z","close_reason":"Obsolete - Newton refinement replaced by golden section + parabolic interpolation (Brent), which is more stable"}
{"id":"jamma-f00","title":"Optimize JAX chunking for large sample counts","description":"At 200K samples, chunk size drops to 1,416 SNPs (68 chunks for 95K SNPs). Each chunk incurs overhead for CPU rotation, device transfer, and result transfer.\n\nCurrent constraint: Uab array (n_snps, n_samples, 6) must stay under 1.7B elements to avoid int32 overflow.\n\nPotential optimizations:\n1. Stream Uab computation - don't store full array, compute Pab incrementally\n2. Fuse rotation with Uab computation to avoid UtG intermediate\n3. Process chunks in parallel threads (multiple JAX streams)\n4. Investigate JAX int64 indexing options (XLA_FLAGS)\n\nBenchmark data (old run, before scipy eigendecomp fix):\n- 10K samples: 78 SNPs/sec (vs GEMMA 891 SNPs/sec) - 11x slower\n- Chunk sizes: 200K→1,416, 100K→2,833, 50K→5,666, 10K→28,333","status":"closed","priority":2,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T01:20:04.780291Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:16.632604Z","closed_at":"2026-02-05T23:39:16.632604Z","close_reason":"Implemented - _compute_chunk_size() and auto_tune_chunk_size() in runner_jax.py"}
{"id":"jamma-irh","title":"Add dimension sanity checks for corrupted PLINK files","description":"Input validation in plink.py - check genotype dimensions match .fam/.bim counts, validate value ranges (0,1,2,NaN only), warn on suspicious patterns. bed-reader handles parsing but not semantic validation.","status":"open","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:42.449529Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.559595Z"}
{"id":"jamma-iw5","title":"Add second public validation dataset (1000 Genomes subset)","description":"Strengthen equivalence story with second dataset beyond mouse_hs1940. Use 1000 Genomes subset (publicly available, no data agreements). Compare stats + p-value distributions vs GEMMA.","status":"closed","priority":3,"issue_type":"task","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:56.285063Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:42:26.402163Z","closed_at":"2026-02-05T23:42:26.402163Z","close_reason":"Existing validation (mouse_hs1940 + synthetic + formal proofs) sufficient"}
{"id":"jamma-kdd","title":"Add benchmark harness with reproducible perf comparisons","description":"Create experiments/benchmarks/ with scripts to compare JAMMA vs GEMMA/GCTA/fastGWA. Include frozen data subsets, automated plots (time vs samples, time vs SNPs). Requires access to hardware with sufficient memory for meaningful benchmarks.","status":"closed","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:51.386942Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:42:26.497377Z","closed_at":"2026-02-05T23:42:26.497377Z","close_reason":"Databricks benchmark notebook covers this; formal harness is overengineering"}
{"id":"jamma-ls1","title":"BIMBAM format input (-g, -p, -a)","description":"Support GEMMA's native BIMBAM mean genotype format. -g: genotype file (mean genotype format), -p: phenotype file, -a: annotation file. Legacy format but still used in some pipelines. Low-medium complexity: new I/O reader, maps to same internal representation as PLINK.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:46.207798Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:46.207798Z"}
{"id":"jamma-qfk","title":"Add top-level jamma.gwas() convenience API","description":"Single blessed entry point: from jamma import gwas. Wraps kinship computation + LMM association in one call. Reduces onboarding friction. Keep existing granular API for advanced users.","status":"closed","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:53:52.813156Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:42:26.214591Z","closed_at":"2026-02-05T23:42:26.214591Z","close_reason":"CLI-first tool, no library consumers need convenience API"}
{"id":"jamma-qqr","title":"Multiple kinship matrices (-mk)","description":"Support loading multiple pre-computed kinship matrices for multi-component variance models. GEMMA uses -mk [num] followed by multiple -k flags. Medium complexity: modifies eigendecomposition and likelihood to handle sum of variance components. Related to variance component estimation (-vc).","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:50.628625Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:50.628625Z","dependencies":[{"issue_id":"jamma-qqr","depends_on_id":"jamma-678","type":"blocks","created_at":"2026-02-06T01:03:07.987249Z","created_by":"Michael Denyer"}]}
{"id":"jamma-qth","title":"Add covariate support to JAX runner","description":"The JAX runners (run_lmm_association_jax and run_lmm_association_streaming) only support n_cvt=1 (intercept only). The NumPy runner handles arbitrary covariates via the general Uab/Pab path.\n\nThe JAX likelihood functions (compute_uab_jax, calc_pab_jax) have fast-path implementations for n_cvt=1 only. Need general n_cvt versions to support -c covariate flag from CLI.\n\nWork needed:\n1. Generalize compute_uab_jax() for arbitrary n_cvt\n2. Generalize calc_pab_jax() for arbitrary n_cvt  \n3. Add covariates parameter to run_lmm_association_jax/streaming\n4. Validate against GEMMA covariate fixtures\n5. Benchmark: ensure n_cvt=1 fast path is preserved (no regression)","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-05T10:54:03.476729Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:06.11757Z","closed_at":"2026-02-05T23:39:06.11757Z","close_reason":"Implemented - JAX runner fully supports covariates via _build_covariate_matrix() and UtW rotation"}
{"id":"jamma-rqz","title":"Make streaming JAX runner the CLI default","description":"The CLI (cli.py lmm_command) currently calls run_lmm_association() — the NumPy per-SNP loop with Brent optimization. This is the slowest path. The benchmark notebook uses run_lmm_association_jax() which is 4x faster.\n\nThe streaming JAX runner (run_lmm_association_streaming) should be the CLI default because:\n- Bounded memory: O(n² + n×chunk) vs O(n² + n×p) for full-load\n- JAX vmap batch processing: parallelizes across SNPs within each chunk\n- No int32 buffer overflow risk (chunk size capped at MAX_SAFE_CHUNK=50k)\n- Incremental disk writes already supported\n\nWork needed:\n1. Depends on: JAX runner supporting all test modes\n2. Switch cli.py lmm_command to call run_lmm_association_streaming()\n3. Add --runner flag to CLI for manual override (numpy/jax/streaming)\n4. Keep run_lmm_association() as fallback for edge cases (tiny datasets, debugging)\n5. Update README performance claims to clarify which runner is used\n6. Update benchmark notebook to also test streaming path\n7. Demote run_lmm_association() from primary to fallback in __init__.py exports","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-05T10:53:58.515713Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:22.865397Z","closed_at":"2026-02-05T23:39:22.865397Z","close_reason":"Implemented - streaming JAX runner is the default CLI path at cli.py:396","dependencies":[{"issue_id":"jamma-rqz","depends_on_id":"jamma-2dj","type":"blocks","created_at":"2026-02-05T10:54:07.718341Z","created_by":"Michael Denyer"},{"issue_id":"jamma-rqz","depends_on_id":"jamma-qth","type":"blocks","created_at":"2026-02-05T10:54:07.873268Z","created_by":"Michael Denyer"}]}
{"id":"jamma-rwo","title":"Add grid search fallback for lambda optimization","description":"When Brent optimization fails or hits bounds, fall back to evaluating likelihood at discrete lambda values (10^-5 to 10^5 in log steps). Useful for diagnostics and pathological cases. pyGEMMA has grid=True option.","status":"closed","priority":4,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:36.21734Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:39:18.178799Z","closed_at":"2026-02-05T23:39:18.178799Z","close_reason":"Implemented - golden_section_optimize_lambda() uses grid bracketing + refinement in likelihood_jax.py"}
{"id":"jamma-sr7","title":"Phenotype column selection (-n)","description":"Implement -n flag to select specific phenotype columns from .fam file (or separate phenotype file). GEMMA uses 1-based column indices. Enables multi-phenotype workflows without pre-processing. Foundation for mvLMM.","status":"open","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:24.570648Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:24.570648Z"}
{"id":"jamma-vcx","title":"LOCO (Leave-One-Chromosome-Out) kinship","description":"Implement -loco flag for LOCO kinship computation. Excludes SNPs on the test chromosome when computing kinship matrix, reducing proximal contamination. Standard practice in production GWAS. Requires chromosome annotation from .bim file.","status":"open","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:17.09739Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:17.09739Z"}
{"id":"jamma-wr2","title":"Wire -maf/-miss filters in CLI","description":"README documents -maf and -miss CLI options but they aren't exposed in cli.py. The filtering logic exists (maf_threshold/miss_threshold params in run_lmm_association) but CLI doesn't wire them up.","status":"closed","priority":3,"issue_type":"bug","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-02T03:19:49.736363Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.555726Z","closed_at":"2026-02-02T03:21:01.295041Z","close_reason":"Not a bug - -maf and -miss ARE wired in CLI. Verified with 'jamma lmm --help' showing both options."}
{"id":"jamma-xfq","title":"Implement chunked SNP processing for large datasets","description":"Process SNPs in chunks to bound memory usage. Required for 500K SNPs at 200K samples where full array would be 4.8TB. Track progress across chunks, allow resume on failure.","status":"closed","priority":1,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:37.887528Z","created_by":"Michael Denyer","updated_at":"2026-02-02T03:21:19.553693Z","closed_at":"2026-02-01T15:26:43.190349Z","close_reason":"Implemented in runner_jax.py with _compute_chunk_size() and chunked processing loop"}
{"id":"jamma-zn0","title":"SNP subset selection (-snps, -ksnps)","description":"Support SNP subsetting via external list file. -snps: restrict association testing to listed SNPs. -ksnps: restrict kinship computation to listed SNPs. Low complexity: read SNP list, filter .bim index before processing. Useful for targeted analyses and replication studies.","status":"open","priority":3,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-06T01:02:48.316979Z","created_by":"Michael Denyer","updated_at":"2026-02-06T01:02:48.316979Z"}
{"id":"jamma-zq8","title":"Add --precision flag for float32/float64 choice","description":"Add CLI option to choose precision. Float32 halves memory (critical for 200K samples) but may reduce numerical precision. Default to float64 for safety, allow float32 for memory-constrained environments. Hybrid approach: float32 for storage, float64 for REML computation.","status":"closed","priority":2,"issue_type":"feature","owner":"97485362+michael-denyer@users.noreply.github.com","created_at":"2026-02-01T02:51:32.80927Z","created_by":"Michael Denyer","updated_at":"2026-02-05T23:42:26.30958Z","closed_at":"2026-02-05T23:42:26.30958Z","close_reason":"Float32 contradicts GEMMA-equivalence value prop"}
