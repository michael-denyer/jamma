---
phase: 18-correctness-performance
plan: 02
type: execute
wave: 2
depends_on: ["18-01"]
files_modified:
  - src/jamma/lmm/runner_jax.py
autonomous: true

must_haves:
  truths:
    - "Result arrays are pre-allocated to full SNP count before the chunk loop"
    - "Results are written by index into pre-allocated arrays, not appended to lists"
    - "No jnp.concatenate call remains for result accumulation"
    - "All existing tests pass with identical statistical output"
  artifacts:
    - path: "src/jamma/lmm/runner_jax.py"
      provides: "Pre-allocated result arrays with index-based writes"
      contains: "np.empty"
  key_links:
    - from: "src/jamma/lmm/runner_jax.py"
      to: "_build_results_wald/_build_results_lrt/_build_results_score/_build_results_all"
      via: "Pre-allocated numpy arrays passed directly (no concatenation step)"
      pattern: "write_offset.*\\+.*slice_len"
---

<objective>
Replace list-append + jnp.concatenate result accumulation in runner_jax.py with pre-allocated numpy arrays and index-based writes.

Purpose: Eliminates list growth overhead, removes the jnp.concatenate + np.asarray double-transfer step, and makes memory usage predictable from the start of the run. This is PERF-02.
Output: Refactored runner_jax.py with pre-allocated result arrays and a write_offset tracking pattern.
</objective>

<execution_context>
@/Users/mdenyer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mdenyer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-correctness-performance/18-RESEARCH.md
@.planning/phases/18-correctness-performance/18-01-SUMMARY.md
@src/jamma/lmm/runner_jax.py
@src/jamma/lmm/results.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace list accumulators with pre-allocated numpy arrays</name>
  <files>src/jamma/lmm/runner_jax.py</files>
  <action>
This task replaces the mode-aware list accumulators (lines 201-227), the per-chunk append logic (lines 373-403), and the concatenation step (lines 409-436) in `run_lmm_association_jax()`.

**Step 1: Replace accumulator initialization (lines 201-227)**

Replace the mode-specific list initialization with pre-allocated numpy arrays. Insert `write_offset = 0` as the position tracker. Use `n_filtered` (already computed at line 182) as the array size.

```python
# Pre-allocate result arrays (replaces list accumulators)
write_offset = 0

if lmm_mode == 1:  # Wald
    lambdas_out = np.empty(n_filtered, dtype=np.float64)
    logls_out = np.empty(n_filtered, dtype=np.float64)
    betas_out = np.empty(n_filtered, dtype=np.float64)
    ses_out = np.empty(n_filtered, dtype=np.float64)
    pwalds_out = np.empty(n_filtered, dtype=np.float64)
elif lmm_mode == 3:  # Score
    betas_out = np.empty(n_filtered, dtype=np.float64)
    ses_out = np.empty(n_filtered, dtype=np.float64)
    p_scores_out = np.empty(n_filtered, dtype=np.float64)
elif lmm_mode == 2:  # LRT
    lambdas_mle_out = np.empty(n_filtered, dtype=np.float64)
    logls_mle_out = np.empty(n_filtered, dtype=np.float64)
    p_lrts_out = np.empty(n_filtered, dtype=np.float64)
elif lmm_mode == 4:  # All tests
    lambdas_out = np.empty(n_filtered, dtype=np.float64)
    logls_out = np.empty(n_filtered, dtype=np.float64)
    betas_out = np.empty(n_filtered, dtype=np.float64)
    ses_out = np.empty(n_filtered, dtype=np.float64)
    pwalds_out = np.empty(n_filtered, dtype=np.float64)
    lambdas_mle_out = np.empty(n_filtered, dtype=np.float64)
    logls_mle_out = np.empty(n_filtered, dtype=np.float64)
    p_lrts_out = np.empty(n_filtered, dtype=np.float64)
    p_scores_out = np.empty(n_filtered, dtype=np.float64)
```

**Step 2: Replace per-chunk append logic (lines 373-403)**

Replace the `all_*.append(...)` pattern with index-based writes. Compute `slice_len` once, write into pre-allocated arrays at `write_offset:write_offset+slice_len`, then increment `write_offset`.

```python
# Write results into pre-allocated arrays by index
if lmm_mode == 1:
    slice_len = actual_chunk_len if needs_padding else len(best_lambdas)
    end = write_offset + slice_len
    lambdas_out[write_offset:end] = np.asarray(best_lambdas[:slice_len])
    logls_out[write_offset:end] = np.asarray(best_logls[:slice_len])
    betas_out[write_offset:end] = np.asarray(betas[:slice_len])
    ses_out[write_offset:end] = np.asarray(ses[:slice_len])
    pwalds_out[write_offset:end] = np.asarray(p_walds[:slice_len])
    write_offset = end
elif lmm_mode == 3:
    slice_len = actual_chunk_len if needs_padding else len(betas)
    end = write_offset + slice_len
    betas_out[write_offset:end] = np.asarray(betas[:slice_len])
    ses_out[write_offset:end] = np.asarray(ses[:slice_len])
    p_scores_out[write_offset:end] = np.asarray(p_scores[:slice_len])
    write_offset = end
elif lmm_mode == 2:
    slice_len = actual_chunk_len if needs_padding else len(best_lambdas_mle)
    end = write_offset + slice_len
    lambdas_mle_out[write_offset:end] = np.asarray(best_lambdas_mle[:slice_len])
    logls_mle_out[write_offset:end] = np.asarray(best_logls_mle[:slice_len])
    p_lrts_out[write_offset:end] = np.asarray(p_lrts[:slice_len])
    write_offset = end
elif lmm_mode == 4:
    slice_len = actual_chunk_len if needs_padding else len(best_lambdas)
    end = write_offset + slice_len
    lambdas_out[write_offset:end] = np.asarray(best_lambdas[:slice_len])
    logls_out[write_offset:end] = np.asarray(best_logls[:slice_len])
    betas_out[write_offset:end] = np.asarray(betas[:slice_len])
    ses_out[write_offset:end] = np.asarray(ses[:slice_len])
    pwalds_out[write_offset:end] = np.asarray(p_walds[:slice_len])
    lambdas_mle_out[write_offset:end] = np.asarray(best_lambdas_mle[:slice_len])
    logls_mle_out[write_offset:end] = np.asarray(logls_mle[:slice_len])
    p_lrts_out[write_offset:end] = np.asarray(p_lrts[:slice_len])
    p_scores_out[write_offset:end] = np.asarray(p_scores[:slice_len])
    write_offset = end
```

IMPORTANT: For mode 4 LRT, `best_logls_mle` is the variable name (not `logls_mle`). Check the actual variable names in the computation block (lines 301-349) carefully -- use the exact same names that the try block produces.

**Step 3: Replace concatenation step (lines 409-436)**

Delete the entire `jnp.concatenate` + `np.asarray` block. The arrays are already numpy and fully populated. Add a defensive assertion after the chunk loop:

```python
assert write_offset == n_filtered, (
    f"Pre-allocated array size mismatch: wrote {write_offset}, expected {n_filtered}"
)
```

Then rename the `*_out` arrays to match the variable names expected by the `_build_results_*` calls below. The simplest approach: use the `*_out` names directly in the `_build_results_*` calls. Update the calls at lines 457-498:

For mode 1 (`_build_results_wald`): pass `lambdas_out, logls_out, betas_out, ses_out, pwalds_out`
For mode 3 (`_build_results_score`): pass `betas_out, ses_out, p_scores_out`
For mode 2 (`_build_results_lrt`): pass `lambdas_mle_out, p_lrts_out`
For mode 4 (`_build_results_all`): pass `lambdas_out, logls_out, betas_out, ses_out, pwalds_out, lambdas_mle_out, p_lrts_out, p_scores_out`

**Step 4: Update the explicit cleanup (lines 438-440)**

Remove the `del all_lambdas, all_logls, ...` lines since those list variables no longer exist. The `del eigenvalues, UtW_jax, Uty_jax` line stays.

**Step 5: Update jax.block_until_ready calls (lines 442-449)**

These currently call `jax.block_until_ready` on the concatenated numpy arrays. Since the arrays are now plain numpy (written via `np.asarray` per chunk), `jax.block_until_ready` is no longer needed -- numpy arrays don't have pending async ops. Remove the `jax.block_until_ready` block entirely, OR keep it as a no-op safety net (it's harmless on numpy arrays). Recommendation: remove it to avoid confusion, since the arrays are demonstrably numpy by construction.

**Anti-patterns to avoid:**
- Do NOT pre-allocate JAX device arrays (`jnp.zeros`). Use numpy `np.empty`.
- Do NOT change `_build_results_*` function signatures in results.py.
- Do NOT modify the streaming runner (runner_streaming.py) -- its dict-based accumulators are scoped to file chunks and are already bounded. Per RESEARCH.md, PERF-02 targets runner_jax.py only.
- Do NOT change the double-buffering pattern (_prepare_chunk / async device_put). That's unchanged.
- Watch for off-by-one: `write_offset` must equal `n_filtered` exactly after the loop.

**Note on mode 4 logls_mle variable:**
In the mode 4 computation block (line 322), the variable is `best_logls_mle` (same name as mode 2). However, `_build_results_all` at line 486 does NOT take `logls_mle` as a parameter -- check the current call signature. If `logls_mle` is not passed to `_build_results_all`, then `logls_mle_out` is still needed for the LRT p-value computation but NOT for the final result builder. In that case, you can omit `logls_mle_out` from the mode 4 pre-allocation IF it's only used intermediately in the try block. Verify by reading `_build_results_all` in results.py. Actually -- looking at the current code, `all_logls_mle` is accumulated in mode 2 (line 214) but never passed to `_build_results_lrt` (line 478 only passes `lambdas_mle_np, p_lrts_np`). So for mode 2 AND mode 4, `logls_mle_out` is dead -- it's accumulated but never consumed. Still pre-allocate it to maintain behavioral parity, but add a comment noting it's unused by result builders.
  </action>
  <verify>
Run the full test suite: `uv run pytest tests/ -x`
Expected: All tests pass with no numerical changes.

Additionally verify no list accumulators remain:
`grep -n "all_lambdas\|all_logls\|all_betas\|all_ses\|all_pwalds\|all_p_scores\|all_p_lrts\|all_lambdas_mle\|all_logls_mle" src/jamma/lmm/runner_jax.py`
Expected: No matches.

Verify no jnp.concatenate for result accumulation:
`grep -n "jnp.concatenate" src/jamma/lmm/runner_jax.py`
Expected: No matches.

Verify pre-allocation exists:
`grep -n "np.empty" src/jamma/lmm/runner_jax.py`
Expected: Matches for each pre-allocated array.

Verify write_offset assertion:
`grep -n "write_offset == n_filtered" src/jamma/lmm/runner_jax.py`
Expected: One match (the assertion after the chunk loop).
  </verify>
  <done>
runner_jax.py uses pre-allocated numpy arrays (`np.empty`) for all result accumulators. Results are written by index using `write_offset` tracking. No `list.append()` or `jnp.concatenate` remains for result accumulation. `assert write_offset == n_filtered` validates completeness after the chunk loop. All existing tests pass with identical statistical output.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -x` -- full suite passes, no regressions
2. `grep -n "all_lambdas\|all_logls\|all_betas\|all_ses\|all_pwalds" src/jamma/lmm/runner_jax.py` -- no list accumulators
3. `grep -n "jnp.concatenate" src/jamma/lmm/runner_jax.py` -- no concatenation calls
4. `grep -n "np.empty" src/jamma/lmm/runner_jax.py` -- pre-allocated arrays present
5. `grep -n "write_offset" src/jamma/lmm/runner_jax.py` -- index tracking present with assertion
</verification>

<success_criteria>
- Zero list-based accumulators in runner_jax.py (no `all_lambdas = []` patterns)
- Zero `jnp.concatenate` calls for result arrays in runner_jax.py
- Pre-allocated `np.empty` arrays for every result field per mode
- `write_offset` tracks position and asserts equality with `n_filtered` after loop
- All existing tests pass with identical statistical output (no numerical changes)
</success_criteria>

<output>
After completion, create `.planning/phases/18-correctness-performance/18-02-SUMMARY.md`
</output>
