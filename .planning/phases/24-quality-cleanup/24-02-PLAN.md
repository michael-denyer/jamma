---
phase: 24-quality-cleanup
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_missingness.py
  - src/jamma/core/memory.py
autonomous: true

must_haves:
  truths:
    - "Tests validate heterogeneous missingness patterns across SNPs and samples"
    - "Tests validate all-missing and near-all-missing SNP edge cases"
    - "Memory model comments in memory.py reflect current dtype assumptions and streaming architecture"
  artifacts:
    - path: "tests/test_missingness.py"
      provides: "Missingness pattern tests for SNP filtering, imputation, and LMM edge cases"
      contains: "test_heterogeneous_missingness"
    - path: "tests/test_missingness.py"
      provides: "All-missing and near-all-missing SNP edge case tests"
      contains: "test_all_missing_snp"
    - path: "src/jamma/core/memory.py"
      provides: "Updated comments reflecting streaming architecture and actual dtype usage"
  key_links:
    - from: "tests/test_missingness.py"
      to: "jamma.core.snp_filter"
      via: "Tests exercise compute_snp_stats and compute_snp_filter_mask"
      pattern: "from jamma\\.core\\.snp_filter import"
    - from: "tests/test_missingness.py"
      to: "jamma.lmm"
      via: "Tests verify LMM handles degenerate SNPs correctly"
      pattern: "NaN|nan"
---

<objective>
Add missingness pattern tests and update stale memory model comments.

Purpose: Current tests use uniform missingness patterns (same rate per SNP). Real GWAS data has heterogeneous missingness — some SNPs have 0% missing, others 20%, and some are entirely missing. The codebase handles these cases (returning NaN for degenerate SNPs per CLAUDE.md), but there are no tests proving it. The memory.py comments also reference stale assumptions after Phase 23 coupled chunk sizes to runtime.

Output: New test file covering missingness edge cases, updated memory.py comments.
</objective>

<execution_context>
@/Users/mdenyer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mdenyer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@src/jamma/core/snp_filter.py
@src/jamma/core/memory.py
@src/jamma/lmm/chunk.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add heterogeneous missingness and edge case tests</name>
  <files>tests/test_missingness.py</files>
  <action>
Create tests/test_missingness.py with two test classes, all marked @pytest.mark.tier0 (no I/O, no GEMMA reference).

**Class 1: TestHeterogeneousMissingness** (TEST-01)

Tests that exercise varying missing rates across SNPs and samples:

1. `test_heterogeneous_missingness_snp_stats`: Create genotype matrix (50 samples, 10 SNPs) where:
   - SNP 0: 0% missing
   - SNP 1: 5% missing (random samples)
   - SNP 2: 20% missing (random samples)
   - SNP 3: 50% missing
   - SNP 4-9: varying rates 0-30%
   Call `compute_snp_stats()` and verify:
   - miss_counts match expected per-SNP counts
   - col_means are computed on non-missing values only (verify manually for SNP 0 and SNP 3)
   - col_vars are non-negative for all SNPs

2. `test_heterogeneous_missingness_filter_mask`: Same genotype matrix. Apply filter with miss_threshold=0.25. Verify:
   - SNP 3 (50% missing) is excluded
   - SNPs with <=25% missing are retained (if they also pass MAF)
   - Returned mask shape matches n_snps

3. `test_high_missingness_sample_pattern`: Create matrix where specific SAMPLES have high missingness (e.g., sample 0 missing for 80% of SNPs, sample 1 missing for 0%). Verify compute_snp_stats still produces correct per-SNP statistics (sample missingness shouldn't affect per-SNP calculations beyond the miss count).

4. `test_cross_pattern_missingness`: Create a checkerboard-like pattern where odd samples are missing for even SNPs and vice versa. Verify:
   - Each SNP has ~50% missing rate
   - Mean imputation still computes from the available values
   - Results are deterministic (run twice, compare)

**Class 2: TestMissingSNPEdgeCases** (TEST-02)

5. `test_all_missing_snp_returns_zero_stats`: Create matrix where one entire column is NaN. Call `compute_snp_stats()`. Verify:
   - col_mean for that SNP is 0.0 (nan_to_num behavior)
   - col_var for that SNP is 0.0
   - miss_count equals n_samples

6. `test_all_missing_snp_filtered_out`: Create matrix with one all-NaN column among normal SNPs. Apply `compute_snp_filter_mask()` with default thresholds. Verify:
   - All-missing SNP is filtered out (mask is False)
   - It's filtered by both the polymorphic check (var=0) and the miss_threshold check

7. `test_near_all_missing_snp`: Create matrix where one SNP has only 1 non-missing value (e.g., sample 0 = 1.0, rest NaN). Verify:
   - compute_snp_stats returns mean = 1.0, var = 0.0 (single value has zero variance)
   - SNP is filtered out by polymorphic check (var=0)
   - miss_count = n_samples - 1

8. `test_near_all_missing_two_values`: Create SNP with only 2 non-missing values (e.g., 0.0 and 2.0). Verify:
   - compute_snp_stats returns mean = 1.0, var = 1.0
   - If miss_threshold allows it (e.g., 1.0), SNP passes filter
   - Behavior is well-defined even with minimal data

9. `test_all_snps_all_missing`: Edge case where EVERY SNP is all-NaN. Call compute_snp_filter_mask. Verify:
   - Mask is all False
   - No errors raised
   - allele_freqs and mafs are all 0.0

All tests use `np.testing.assert_array_equal` or `assert_allclose` for numerical checks. Use `np.random.default_rng(seed)` for reproducibility.
  </action>
  <verify>
```bash
uv run pytest tests/test_missingness.py -x -v
```
All tests pass.
  </verify>
  <done>9 tests covering heterogeneous missingness patterns and all-missing/near-all-missing edge cases. All pass.</done>
</task>

<task type="auto">
  <name>Task 2: Update memory.py comments to reflect streaming architecture</name>
  <files>src/jamma/core/memory.py</files>
  <action>
Update stale comments and docstrings in memory.py:

1. **estimate_workflow_memory docstring** (line 104-108): The Note says "conservative estimate based on a fixed batch size. The JAX runner computes its own chunk size..." This is stale after Phase 23 (23-03) which coupled memory estimation to the actual chunk size. Update to reflect that `check_memory_before_run()` now calls `_compute_chunk_size()` for the actual JAX chunk size, while `estimate_workflow_memory()` accepts `lmm_batch_size` as a parameter for direct callers.

2. **genotypes_gb comment** (lines 127, 205): Currently says `float64 (JAX copy)`. In the streaming architecture, genotypes are never fully materialized — they're read as chunks. The full-load estimate in `estimate_workflow_memory` should note this is the full-materialization estimate (non-streaming path), while `estimate_streaming_memory` correctly uses chunk_size. Add a brief note: "Full materialization — streaming path uses chunk_size instead (see estimate_streaming_memory)."

3. **MemoryBreakdown docstring** (lines 56-69): Says "Peak memory is the maximum of eigendecomp phase and LMM phase since they don't overlap." This is correct but should mention that the streaming variant (`StreamingMemoryBreakdown`) is preferred for production use since streaming is now the sole execution path.

4. **estimate_lmm_memory docstring** (lines 182-198): References "lmm_batch_size" but the default is 20_000 which doesn't reflect any real default. Add note that `auto_tune_chunk_size()` in `chunk.py` computes the actual runtime chunk size and callers should use that value.

Keep changes to comments/docstrings only — NO functional changes to any code.
  </action>
  <verify>
```bash
uv run pytest tests/test_memory.py tests/test_memory_chunk_coupling.py -x
```
Tests pass (no functional changes, only comments).
  </verify>
  <done>Memory model comments in memory.py reflect current streaming-only architecture, actual chunk size coupling, and dtype assumptions.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_missingness.py -x -v` — all 9 missingness tests pass
2. `uv run pytest tests/test_memory.py tests/test_memory_chunk_coupling.py -x` — existing memory tests pass
3. `grep -c "test_" tests/test_missingness.py` — at least 9 test functions
4. `grep "streaming" src/jamma/core/memory.py` — updated comments reference streaming architecture
</verification>

<success_criteria>
- Heterogeneous missingness tests cover: varying rates per SNP, high-missingness samples, cross-pattern missingness
- Edge case tests cover: all-missing SNP, near-all-missing (1 value), near-all-missing (2 values), all-SNPs-all-missing
- Memory.py comments accurately describe streaming-only architecture and chunk size coupling
- All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/24-quality-cleanup/24-02-SUMMARY.md`
</output>
